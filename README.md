# Spark_course

### **Spark Session:**

- This code shows how to create a Spark session and a Spark context using PySpark

### **RDDs exercises:**

- This code shows how to work with RDDs in PySpark.
- I performed several operations with RDDs, such as generating an RDD from a list, applying transformations like map and filter, and performing actions like reduceByKey and countByValue.

### **DataFrames SQL exercises:**

- This code shows how to work with DataFrames in PySpark.
- I have performed several operations with DataFrames, such as importing a CSV file, viewing data, displaying the schema, counting records, getting descriptive statistics, selecting and filtering data, and using Spark SQL.

### **Data Analytics exercise:**

- This code shows how to work with DataFrames in PySpark and how to integrate them with the data visualisation libraries.
- I have performed several operations with DataFrames, such as defining a schema, importing a CSV file, displaying the schema, deleting a column, filtering and deleting records, grouping and calculating descriptive statistics, converting a Spark DataFrame into a pandas DataFrame, calculating the average of columns grouped by another column, generating charts with matplotlib and seaborn, saving data in a parquet file and reading a parquet file as a DataFrame.

### **Koalas exercise:**

- This code shows how to work with the koala databricks library.
- I have performed several basic operations with Koalas, such as creating series and DataFrames, converting pandas objects to Koalas objects and sorting series by index, I have performed visualisation and data selection operations with Koalas DataFrames, also visualisation and data selection operations with Koalas DataFrames. I have applied Python functions to Koalas DataFrames and series. Furthermore, I have grouped data and calculated descriptive statistics. I have generated graphs with matplotlib. I have used SQL in Koalas. In addition, I have worked with PySpark and Spark DataFrames.
  
### **Machine Learning exercises:**

- This code shows how to work with the pandas profiling library and how to use the PySpark machine learning library to train a binary classification model.
- I have performed several operations with DataFrames, such as importing a CSV file, converting a Spark DataFrame into a pandas DataFrame, generating a data profiling report, defining a feature assembler, transforming the DataFrame, splitting it into training and test sets, defining and training a model, making a prediction and evaluating the model.

### **Prediction Model Training:**

- This code shows how to use the PySpark machine learning library to train a binary classification model, and how to use a pipeline to chain together various stages of pre-processing and training the model.
- I have performed several operations with DataFrames, such as importing a CSV file, renaming a column, splitting the DataFrame into training and test sets, defining a model, a hot encoder, two feature assemblers and a scaler, setting up a multi-stage pipeline, fitting the model with the training data, transforming the data and saving the fitted model.

### **Saprk Streaming exercise:**

- This code shows how to use a pre-trained machine learning model to make streaming predictions with PySpark.
- I have performed several operations with DataFrames, such as importing a CSV file, splitting the DataFrame into training and test sets, loading a previously trained model, transforming the data, selecting and displaying columns, writing the DataFrame to a directory, defining a streaming data source, transforming the streaming data source, and displaying or saving the streaming predictions.
